{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4 (Part C) : Random Forest\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "\n",
    "__IMPORTANT__ \n",
    "Please complete this Jupyter Notebook file and upload it to blackboard __before 20 February 2020__.\n",
    "</div>\n",
    "\n",
    "In this part of the Lab, you will implement a very simplified version of the Random Forest classifier. Random Forst (RF) works as follows:\n",
    "\n",
    "- Given a dataset $X \\in \\mathbb{R}^{n \\times d}$, RF selects randomly some features (i.e. `n_features`) from the dataset (where `n_features << d`). It also selects a random subset of the data (with `n_samples` data-points). Then, it builds a Decision Tree from those selected features and samples.\n",
    "\n",
    "- Repeats this process `n_trees` times so that you have a number of `n_trees` Decision Trees built from different random combinations of features and different random subsets of data.\n",
    "\n",
    "- To predict, RF takes each of the `n_trees` built Decision Trees and predict the outputs (classes); then it calculates the votes for each predicted class-label and takes the mode (most frequent label). In other words, considers the high voted predicted label as the final prediction from the random forest algorithm.\n",
    "\n",
    "## Loading the dataset:\n",
    "The code below will load a training dataset into variables `X` and `y` as usual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[[34.62365962 78.02469282]\n [30.28671077 43.89499752]\n [35.84740877 72.90219803]] [0. 0. 0.]\n"
    }
   ],
   "source": [
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "\n",
    "filename = \"datasets/university-admission-dataset.csv\"\n",
    "mydata = np.genfromtxt(filename, delimiter=\",\")\n",
    "X = mydata[:, :2]\n",
    "y = mydata[:, -1]\n",
    "\n",
    "\"\"\" TODO:\n",
    "Print a small subset of X and y to see how the data looks like.\n",
    "\"\"\"\n",
    "print(X[:3], y[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using sklearn.tree.DecisionTreeClassifier\n",
    "To simplify the implementation of our Random Forest classifier, we will use an existing implementation of the `DecisionTreeClassifier` available in the sklearn library. Read the following code to see an example of how to use the `DecisionTreeClassifier`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Predictions: [0. 0. 0. 1. 1. 0. 1. 1. 1. 1. 0. 0. 1. 1. 0. 1. 1. 0. 1. 1. 0. 1. 0. 0.\n 1. 1. 1. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 1. 1.\n 1. 1. 1. 1. 1. 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 1.\n 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 0. 1. 1. 1.\n 1. 1. 1. 1.]\nTraining Accuracy: 100.0%\n"
    }
   ],
   "source": [
    "\"\"\" TODO:\n",
    "Read and run the following example code to see how to use a \n",
    "simple DecisionTreeClassifier to make predictions.\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "clf = DecisionTreeClassifier(random_state=0).fit(X, y) # Training\n",
    "preds = clf.predict(X) # Predicting\n",
    "\n",
    "print(\"Predictions:\", preds)\n",
    "\n",
    "acc = np.mean(preds == y) * 100\n",
    "print(\"Training Accuracy: {}%\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing a simplified random forest classifier\n",
    "Complete the code below to implement a random forest classifier.\n",
    "\n",
    "In Python, to select a list of `k` random integers between `0` and `nbr` (excluded), you can use `ids = np.random.choice(nbr, k, replace=False)`. The keyword `replace=False` means that we don't want to select the same number more than once (so, ids with contain `k` unique integers).\n",
    "\n",
    "Also, to compute the mode (most common or frequent label), you can make use of the function `scipy.stats.mode` if you want. Read its documentation here: https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.mode.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Training Accuracy: 30.0%\n"
    }
   ],
   "source": [
    "from scipy import stats\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "\"\"\" TODO:\n",
    "Complete the definition of the function train(...) which allows to train an ensemble \n",
    "of decision tree classifiers. Each trained decision tree clf is saved together with \n",
    "its corresponding features indices (used to train clf) into the list clfs.\n",
    "\n",
    "Arguments:\n",
    "*** X: the training dataset.\n",
    "*** y: the training labels corresponding to X.\n",
    "*** n_features: the number of features to select randomly to build a decision tree.\n",
    "*** n_samples: the number of instances to select randomly to build a decision tree.\n",
    "*** n_trees: the number of decision trees to build.\n",
    "\"\"\"\n",
    "def train(X, y, n_features, n_samples, n_trees):\n",
    "    n, d = X.shape # n: number of rows, and d: number of columns\n",
    "    clfs = [] # used to store the decision trees with their corresponding features indices.\n",
    "    \n",
    "    for itr in range(n_trees):\n",
    "        # TODO: select randomly n_features indices between 0 and d and store them in ids_columns\n",
    "        ids_columns = np.random.choice(d, n_features, replace=False) # you can use np.random.choice here\n",
    "        \n",
    "        # TODO: select randomly n_samples indices between 0 and n and store them in ids_samples\n",
    "        ids_samples = np.random.choice(n, n_samples, replace=False) # you can use np.random.choice here\n",
    "        \n",
    "        # TODO: select the subset of X corresponding to the rows ids_samples and the columns ids_columns\n",
    "        Xsub = [x[ids_columns] for x in X[ids_samples]]\n",
    "        \n",
    "        # TODO: select the subset of y corresponding to ids_samples\n",
    "        ysub = y[ids_samples]\n",
    "        \n",
    "        # TODO: train a decision tree classifier clf, based on the training subset: Xsub, ysub\n",
    "        clf = DecisionTreeClassifier(random_state=0).fit(Xsub,ysub)\n",
    "        \n",
    "        # TODO: save the trained decision tree classifier (clf) and the corresponding \n",
    "        # feature indices (ids_columns) used to train this classifier, into the list clfs.\n",
    "        clfs.append((clf, ids_columns))\n",
    "        \n",
    "    return clfs\n",
    "\n",
    "\n",
    "\"\"\" TODO:\n",
    "Complete the definition of the function predict(clfs, X) which allows to predict the class-labels \n",
    "for the instances in the given dataset X, using the ensemble clfs returned by your function previous function.\n",
    "\"\"\"\n",
    "def predict(clfs, X):\n",
    "    preds = []\n",
    "    for clf in clfs:\n",
    "        pred = clf[0].predict(X[:,clf[1]])\n",
    "        preds.append(stats.mode(pred, axis=None))\n",
    "    return preds\n",
    "\n",
    "\"\"\" TODO:\n",
    "Uncomment the code below to test your functions:\n",
    "\"\"\"\n",
    "clfs = train(X, y, n_features=1, n_samples=50, n_trees=10)\n",
    "y_pred = predict(clfs, X)\n",
    "acc = np.mean(y_pred == y) * 100\n",
    "print(\"Training Accuracy: {}%\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Testing your implementation a dataset with multiple features\n",
    "This part is optional.\n",
    "\n",
    "Test you random forest implementation on a dataset with more features (e.g. you can add more polynomial features to the previous dataset or use any other dataset with a high number of features).\n",
    "\n",
    "Compute the generalization accuracy of your random forest on that dataset using a 10-fold-cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Avg acc: 27.0%\n"
    }
   ],
   "source": [
    "# TODO (optional) ...\n",
    "import math\n",
    "def polyFeatures(X, p):\n",
    "    return np.array([[pow(x, i) for i in range(1, p+1)] for x in X[:,0]])\n",
    "\n",
    "k = 10\n",
    "p = 8\n",
    "X_poly = polyFeatures(X, p)\n",
    "# Compute partition size given input k\n",
    "#n = len(X)\n",
    "splitLen = int(math.floor(n/float(k)))\n",
    "\n",
    "accs = []\n",
    "for i in range(k):\n",
    "    XSplit = X_poly[i*splitLen:i*splitLen+splitLen]\n",
    "    ySplit = y[i*splitLen:i*splitLen+splitLen]\n",
    "    clfs = train(XSplit, ySplit, 8, 10, 10)\n",
    "    y_pred = predict(clfs, XSplit)\n",
    "    accs.append(np.mean(y_pred == y) * 100)\n",
    "    \n",
    "print(\"Avg acc: {}%\".format(np.mean(accs)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}