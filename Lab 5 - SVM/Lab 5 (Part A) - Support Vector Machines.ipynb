{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 5 (Part A) : Support Vector Machines\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "\n",
    "__IMPORTANT__ \n",
    "Please complete this Jupyter Notebook file and upload it to blackboard __before 28 February 2020__ evening.\n",
    "</div>\n",
    "\n",
    "In this Lab, you will first use support vector machines (SVM) with various example 2D datasets in \"Part A\", then you will have the oportunity to use SVM to build a spam classifier in \"Part B\". Before starting, we strongly recommend reading the slides of lecture 5.\n",
    "\n",
    "In this first part of this Lab (Part A), you will be experimenting SVM with 2D datasets to help you gain an intuition of how SVM work and how to use a Gaussian kernel with SVM.\n",
    "\n",
    "## 1. Example Dataset 1\n",
    "We will begin by with a 2D example dataset (file `lab5data1.mat`) which can be separated by a linear boundary. The following Python code will load and plot the training data.\n",
    "\n",
    "In this dataset, the positions of the positive examples (indicated with `1`) and the negative examples (indicated with `0`) suggest a natural separation indicated by the gap. However, notice that there is an outlier positive example on the far left at about `(0.1, 4.1)`. As part of this exercise, you will also see how this outlier affects the SVM decision boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "\n",
    "# Loading the dataset into X (inputs) and y (outputs)\n",
    "mat = loadmat(\"datasets/lab5data1.mat\")\n",
    "X = mat[\"X\"]\n",
    "y = mat[\"y\"].reshape(len(X))\n",
    "print(\"X.shape:\", X.shape, \"y.shape:\", y.shape)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "X0, X1 = X[y==0], X[y==1]\n",
    "ax.scatter(X0[:, 0], X0[:, 1], marker=\"$0$\", color=\"red\")\n",
    "ax.scatter(X1[:, 0], X1[:, 1], marker=\"$1$\", color=\"blue\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, you will try using different values of the $C$ parameter with SVM. Informally, the $C$ parameter is a positive value that controls the penalty for misclassified training examples. A large $C$ parameter tells the SVM to try to classify all the examples correctly. $C$ plays a role similar to $\\frac{1}{\\lambda}$, where $\\lambda$ is the regularization parameter that we were using previously for logistic regression.\n",
    "\n",
    "In order to understand this, the function `svm_train_and_plot(..)` provided below will run the SVM training (with a given parameter $C = 1$) using the sklearn library in Python. Read and run the code below before continuing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "\"\"\" TODO:\n",
    "Read the following code carefully then run it.\n",
    "\"\"\"\n",
    "def svm_train_and_plot(X, y, C):\n",
    "    clf = SVC(C=C, kernel=\"linear\").fit(X, y) # Training\n",
    "    theta = np.concatenate([clf.intercept_, clf.coef_[0]]) # The parameters vector theta\n",
    "    \n",
    "    # Plotting the dataset and linear decision boundary\n",
    "    fig, ax = plt.subplots()\n",
    "    X0, X1 = X[y==0], X[y==1]\n",
    "    ax.scatter(X0[:, 0], X0[:, 1], marker=\"$0$\", color=\"red\")\n",
    "    ax.scatter(X1[:, 0], X1[:, 1], marker=\"$1$\", color=\"blue\")\n",
    "    \n",
    "    plot_x1 = np.linspace(0, 4)\n",
    "    plot_x2 = - (theta[0] + theta[1] * plot_x1) / theta[2]\n",
    "    ax.plot(plot_x1, plot_x2, color=\"green\")\n",
    "    \n",
    "    ax.set_title(\"SVM Decision Boundary with C = {}\".format(C))\n",
    "    fig.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code below, your task is to call the function `svm_train_and_plot(X, y, C)` and try different values of $C$ on this dataset (e.g. try a least $C=1$ and $C=100$) and compare the results.\n",
    "\n",
    "**Note**: Most SVM libraries (including sklearn.svm.LinearSVC) automatically add the extra feature $x_0 = 1$ for you and automatically take care of learning the intercept term $\\theta_0$. So when passing your training data to train SVM, there is no need to add this extra feature $x_0 = 1$ yourself. In particular, in Python your code should be working with training examples $x \\in \\mathbb{R}^d$ (rather than $x \\in \\mathbb{R}^{d+1}$); for example, in the first example dataset $x \\in \\mathbb{R}^{2}$.\n",
    "\n",
    "When $C = 1$, you should find that the SVM puts the decision boundary in the gap between the two datasets and misclassifies the outlier data-point on the far left (see the left plot in the following figure). When $C = 100$, you should find that the SVM now classifies every single example correctly, but has a decision boundary that does not\n",
    "appear to be a natural fit for the data - it overfits (see the right plot in the following figure).\n",
    "\n",
    "<img src=\"imgs/svmData1CLab5.png\" width=\"800px\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\" TODO:\n",
    "Call svm_train_and_plot(X, y, C) with different values of C (e.g. 1, 10, 100) and compare the resulting plots.\n",
    "\"\"\"\n",
    "# svm_train_and_plot(...)\n",
    "# svm_train_and_plot(...)\n",
    "# svm_train_and_plot(...)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. SVM with Gaussian Kernels\n",
    "In this part of the exercise, you will be using SVM to do nonlinear classification. In particular, you will be using SVM with Gaussian kernels on datasets that are not linearly separable.\n",
    "\n",
    "### 2.1. Gaussian Kernel\n",
    "To find nonlinear decision boundaries with the SVM, we need to first implement a Gaussian kernel. You can think of the Gaussian kernel as a similarity function between a pair of examples $(x^{(i)}, x^{(j)})$. The Gaussian kernel is also parameterized by a bandwidth parameter, $\\sigma$, which determines how fast the similarity metric decreases (to $0$) as the examples are further apart (or more distant). Note: we have seen how this Gaussian kernel function looks like when we studied Kernel Regression un a previous lecture.\n",
    "\n",
    "You should now complete the code below to compute the Gaussian kernel between two examples, $(x^{(i)}, x^{(j)})$. The Gaussian kernel function is defined as:\n",
    "\n",
    "$$\n",
    "K_{\\text{gaussian}}(x^{(i)}, x^{(j)}) = \\exp{\\left (-\\frac{{\\left \\| x^{(i)} - x^{(j)} \\right \\|}^2}{2 \\sigma^2} \\right )} = \\exp{ \\left ( - \\frac{ \\sum_{k=1}^{n} (x_k^{(i)} - x_k^{(j)})^2 }{2 \\sigma^2} \\right ) }\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" TODO:\n",
    "Write the definition of the gaussian kernel function of between two vectors u and v using a bandwidth sigma.\n",
    "\"\"\"\n",
    "def gaussianKernel(u, v, sigma):\n",
    "    ...\n",
    "    return ...\n",
    "\n",
    "\n",
    "\"\"\" TODO:\n",
    "Test your gaussianKernel(..) function using the two vectors np.array([1,2]), np.array([0,3]) \n",
    "and the bandwidth sigma=1. You should expect to see a value of about 0.36787944.\n",
    "\"\"\"\n",
    "# gaussianKernel(...)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Example Dataset 2\n",
    "The following Python code will load and plot dataset 2. Run the code and check the resulting figure.\n",
    "\n",
    "From the figure, you can obserse that there is no linear decision boundary that separates the positive and negative examples for this dataset. However, by using the Gaussian kernel with SVM, you will be able to learn a nonlinear decision boundary that can perform reasonably well for the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The old (previous) variables X and y will be replaced with the new dataset.\n",
    "\n",
    "mat = loadmat(\"datasets/lab5data2.mat\")\n",
    "X = mat[\"X\"]\n",
    "y = mat[\"y\"].reshape(len(X))\n",
    "print(\"X.shape:\", X.shape, \"y.shape:\", y.shape)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "X0, X1 = X[y==0], X[y==1]\n",
    "ax.scatter(X0[:, 0], X0[:, 1], marker=\"$0$\", color=\"red\")\n",
    "ax.scatter(X1[:, 0], X1[:, 1], marker=\"$1$\", color=\"blue\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make SVM training faster to run, we are not going to use the `gaussianKernel(..)` function that you implemented previously. We will use a faster implementation of the gaussian kernel which is already implemented in `sklearn.svm import SVC`. Now, run the following code where a function `nonlinear_svm_train_and_plot(X, y, C, sigma)` is provided. This function trains a nonlinear SVM on the training dataset `X, y`, using the specified `C` and a gaussian kernel parametrized by the specified `sigma` ($\\sigma$). Just run the code, you DO NOT have to fully understand how the plotting is done inside the function `nonlinear_svm_train_and_plot(..)` but you can still read it and benefit from it if you want.\n",
    "\n",
    "**NOTE:** When the function `nonlinear_svm_train_and_plot(..)` is called, it might take some time (few seconds) before it finishes running and produces the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "def nonlinear_svm_train_and_plot(X, y, C, sigma):\n",
    "    print(\"Please wait. This might take some time (few seconds) ...\")\n",
    "    \n",
    "    gamma = 1 / (2 * sigma**2)\n",
    "    clf = SVC(C=C, kernel=\"rbf\", gamma=gamma).fit(X, y) # Training\n",
    "    \n",
    "    # Plotting the dataset and nonlinear decision boundary\n",
    "    fig, ax = plt.subplots()\n",
    "    X0, X1 = X[y==0], X[y==1]\n",
    "    ax.scatter(X0[:, 0], X0[:, 1], marker=\"$0$\", color=\"red\")\n",
    "    ax.scatter(X1[:, 0], X1[:, 1], marker=\"$1$\", color=\"blue\")\n",
    "    \n",
    "    x1_min, x1_max = np.min(X[:, 0]), np.max(X[:, 0])\n",
    "    x2_min, x2_max = np.min(X[:, 1]), np.max(X[:, 1])\n",
    "    plot_x1, plot_x2 = np.meshgrid(np.arange(x1_min, x1_max, 0.004), np.arange(x2_min, x2_max, 0.004))\n",
    "    Z = clf.predict(np.c_[plot_x1.ravel(), plot_x2.ravel()])\n",
    "    Z = Z.reshape(plot_x1.shape)\n",
    "    \n",
    "    ax.contour(plot_x1, plot_x2, Z, colors=\"green\")\n",
    "    \n",
    "    ax.set_title(\"SVM Decision Boundary with $C = {}, \\sigma = {}$\".format(C, sigma))\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "# We call here our function defined above, with C=1 and sigma = 0.1\n",
    "nonlinear_svm_train_and_plot(X, y, C = 1, sigma = 0.1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code below, your task is to call the function `nonlinear_svm_train_and_plot(X, y, C, sigma)` and try different values of `sigma` on this dataset, e.g. $\\sigma=0.01, \\sigma=0.1$, and \\sigma=0.5$ (you can, of course, try other values as well).\n",
    "\n",
    "The following figure shows the decision boundary found by the SVM with $C=1$ and a Gaussian kernel with $\\sigma = 0.01$, $\\sigma = 0.1$, $\\sigma = 0.5$. When $\\sigma$ is too small (e.g. 0.01 for this dataset), you can see that the decision boundary is very complex and is able to separate all of the positive and negative examples correctly; but migt not generalize well to new unseen data (i.e. overfitting occurs). With $\\sigma = 0.1$, the decision boundary is able to separate most of the positive and negative examples correctly and follows the contours of the dataset well. However, when $\\sigma$ is too large (e.g. 0.5 for this dataset), then it might result in underfitting, as you can see from the following figure (the decision boundary is not able to separate the two classes well enough).\n",
    "\n",
    "<img src=\"imgs/svmData2SigmaLab5.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\" TODO:\n",
    "Call nonlinear_svm_train_and_plot(X, y, C, sigma) with different values of sigma and compare the resulting plots.\n",
    "\"\"\"\n",
    "# nonlinear_svm_train_and_plot(X, y, C = 1, sigma = 0.01)\n",
    "# nonlinear_svm_train_and_plot(X, y, C = 1, sigma = 0.1)\n",
    "# nonlinear_svm_train_and_plot(X, y, C = 1, sigma = 0.5)\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Example Dataset 3\n",
    "In this section, you will gain more practical skills on how to use a SVM with a Gaussian kernel. The following Python code will load and display a third dataset from `lab5data3.mat`. In this provided dataset, you are given the variables `X, y, Xval, yval`. Run the following code to load and display the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The old (previous) variables X and y will be replaced with the new dataset.\n",
    "\n",
    "mat = loadmat(\"datasets/lab5data3.mat\")\n",
    "X = mat[\"X\"]\n",
    "y = mat[\"y\"].reshape(len(X))\n",
    "Xval = mat[\"Xval\"]\n",
    "yval = mat[\"yval\"].reshape(len(Xval))\n",
    "\n",
    "print(\"X.shape:\", X.shape, \"y.shape:\", y.shape)\n",
    "print(\"Xval.shape:\", X.shape, \"yval.shape:\", y.shape)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(9, 4))\n",
    "X0, X1 = X[y==0], X[y==1]\n",
    "ax1.scatter(X0[:, 0], X0[:, 1], marker=\"$0$\", color=\"red\")\n",
    "ax1.scatter(X1[:, 0], X1[:, 1], marker=\"$1$\", color=\"blue\")\n",
    "ax1.set_title(\"The training set (X, y)\")\n",
    "\n",
    "Xval0, Xval1 = Xval[yval==0], Xval[yval==1]\n",
    "ax2.scatter(Xval0[:, 0], Xval0[:, 1], marker=\"o\", color=\"red\")\n",
    "ax2.scatter(Xval1[:, 0], Xval1[:, 1], marker=\"+\", color=\"blue\")\n",
    "ax2.set_title(\"The validation set (Xval, yval)\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, you will be using the SVM with the Gaussian kernel on this dataset. In the code provided below, the function `train_nonlinear_svm(X, y, C, sigma)` trains the SVM classifier using the training set $(X, y)$ and the provided parameters $C$ and $\\sigma$. The function returns an object `clf` that you can use later to make predictions on a new dataset such as `Xval` (using `clf.predict(Xval)`).\n",
    "\n",
    "Your task is to use the validation set `Xval, yval` to determine the best $C$ and $\\sigma$ parameter to use. You should write any additional code necessary to help you search over the parameters $C$ and $\\sigma$. For both $C$ and $\\sigma$, we suggest trying values in multiplicative steps (e.g., 0.01, 0.03, 0.1, 0.3, 1, 3, 10, 30). Note that you should try all possible pairs of values for $C$ and $\\sigma$ (e.g., $C = 0.3$ and $\\sigma = 0.1$). For example, if you try each of the 8 values listed above for $C$ and for $\\sigma$, you would end up training and evaluating (on the cross validation set) a total of $8^2 = 64$ different SVM models.\n",
    "\n",
    "After you have determined the best $C$ and $\\sigma$ parameters to use, call the function `nonlinear_svm_train_and_plot(..)` using the best $C$ and $\\sigma$ to plot the corresponding SVM decision boundary. For our best parameters, the SVM returned a decision boundary shown in the following figure.\n",
    "<img src=\"imgs/bestCandSigmaLab5.png\" width=\"400px\" />\n",
    "\n",
    "**Implementation Tip**: When implementing the hyperparameters tunning to select the best $C$ and $\\sigma$ to use, you need to evaluate the error on the validation set. Recall that for classification, the accuracy is defined as the fraction of the validation examples that were classified correctly. In Python, you can compute this accuracy (in percentage %) using `np.mean(predictions == yval) * 100`, where `predictions` is an array containing all the predictions from the SVM, and `yval` are the true labels from the validation set. You can use `clf.predict(Xval)` to generate the predictions for the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function trains a nonlinear svm model and returns an object clf \n",
    "# that you can use later to make predictions. Read this function:\n",
    "def train_nonlinear_svm(X, y, C, sigma):\n",
    "    gamma = 1 / (2 * sigma**2)\n",
    "    clf = SVC(C=C, kernel=\"rbf\", gamma=gamma).fit(X, y) # Training SVM\n",
    "    return clf\n",
    "\n",
    "\n",
    "\"\"\" TODO:\n",
    "Write code here to use the validation set Xval, yval to determine the best C and sigma parameter to use.\n",
    "Try using values of C and sigma from this range: [0.01, 0.03, 0.1, 0.3, 1, 3, 10, 30].\n",
    "\"\"\"\n",
    "# ...\n",
    "# C_best, sigma_best = ...\n",
    "\n",
    "\n",
    "\"\"\" TODO:\n",
    "After you have determined the best C and sigma parameters to use, \n",
    "call the function nonlinear_svm_train_and_plot(X, y, C_best, sigma_best) \n",
    "using the best C and sigma to plot the corresponding SVM decision boundary.\n",
    "\"\"\"\n",
    "# nonlinear_svm_train_and_plot(X, y, C_best, sigma_best)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
